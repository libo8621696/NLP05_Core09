{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5\n",
      "Loss:  nan\n",
      "of | to, n, the, <unk>, of\n",
      "no | to, n, the, <unk>, of\n",
      "one | to, n, the, <unk>, of\n",
      "first | to, n, the, <unk>, of\n",
      "they | to, n, the, <unk>, of\n",
      "all | to, n, the, <unk>, of\n",
      "as | to, n, the, <unk>, of\n",
      "there | to, n, the, <unk>, of\n",
      "continues | to, n, the, <unk>, of\n",
      "directly | to, n, the, <unk>, of\n",
      "difficult | to, n, the, <unk>, of\n",
      "exports | to, n, the, <unk>, of\n",
      "earned | to, n, the, <unk>, of\n",
      "bankers | to, n, the, <unk>, of\n",
      "quarterly | to, n, the, <unk>, of\n",
      "questions | to, n, the, <unk>, of\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torchnlp.download import download_files_maybe_extract\n",
    "from torchnlp.encoders.text import DEFAULT_EOS_TOKEN\n",
    "from torchnlp.encoders.text import DEFAULT_UNKNOWN_TOKEN\n",
    "\n",
    "\n",
    "def penn_treebank_dataset(\n",
    "    directory='data/penn-treebank',\n",
    "    train=False,\n",
    "    dev=False,\n",
    "    test=False,\n",
    "    train_filename='ptb.train.txt',\n",
    "    dev_filename='ptb.valid.txt',\n",
    "    test_filename='ptb.test.txt',\n",
    "    check_files=['ptb.train.txt'],\n",
    "    urls=[\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "        'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt'\n",
    "    ],\n",
    "    unknown_token=DEFAULT_UNKNOWN_TOKEN,\n",
    "    eos_token=DEFAULT_EOS_TOKEN):\n",
    "\n",
    "    download_files_maybe_extract(urls=urls, directory=directory, check_files=check_files)\n",
    "\n",
    "    ret = []\n",
    "    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n",
    "    splits = [f for (requested, f) in splits if requested]\n",
    "    for filename in splits:\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        text = []\n",
    "        with io.open(full_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                text.extend(line.replace('<unk>', unknown_token).split())\n",
    "                text.append(eos_token)\n",
    "        ret.append(text)\n",
    "\n",
    "    if len(ret) == 1:\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return tuple(ret)\n",
    "\n",
    "## 通过以上的penn_treebank_dataset()导入ptb数据集并放入data/penn-treebank文件夹中\n",
    "penn_treebank_dataset()\n",
    "\n",
    "## 从data/penn-treebank文件夹中的ptb.train.txt读取数据\n",
    "with open('./data/penn-treebank/ptb.train.txt') as f_ptb_train:\n",
    "    text_ptb_train = f_ptb_train.read()\n",
    "\n",
    "# 打印前50个字符做测试\n",
    "# print(text_ptb_train[:100])\n",
    "\n",
    "# 得到如下结果：\n",
    "# aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
    "# 可以看出，这里的只是字符的输出，没有进行分词，因此在生成词向量前需要对文本进行预处理，这里的预处理代码都在utils.py之中，主要的作用是将一些分隔符转换为特殊符号，同时 删除出现不超过5次的单词，减少这些低频词所带来的数据噪声，提升词向量表示的质量，最终会返回词列表。\n",
    "\n",
    "words = utils.preprocess(text_ptb_train)\n",
    "# print(words[:30])\n",
    "\n",
    "#打印出前30个词语如下 ['pierre', '<unk>', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', '<PERIOD>', 'n', 'mr', '<PERIOD>', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n', '<PERIOD>', 'v', '<PERIOD>', 'the', 'dutch', 'publishing']\n",
    "\n",
    "# print(\"Total words in text: {}\".format(len(words)))\n",
    "# print(\"Unique words: {}\".format(len(set(words)))) \n",
    "\n",
    "# 通过`set` 去除重复的词，得到如下结果，训练集合文本共有905112个单词，除去重复的单词共有9515个单词。\n",
    "# Total words in text: 905112 Unique words: 9515\n",
    "\n",
    "# 在utils.py中，通过建立查找表的函数create_lookup_tables得到两个字典，一个字典是将词语映射到出现ID号码，另一个是将ID号码返回到词语本身。ID 号码的大小是根据出现频次大小来逆序排列的，例如the出现次数最多，则编号为0,出现次数次多的则编号为1，依此类推。\n",
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "# print(int_words[:30])\n",
    "# 对应于前30个单词的相应编号为[8510, 1, 2, 74, 394, 33, 2113, 0, 147, 19, 5, 8511, 276, 409, 8, 2, 23, 8, 1, 13, 142, 3, 1, 2, 8, 2460, 8, 0, 3047, 1584]\n",
    "\n",
    "# 二次采样，对于“the”，“of”以及“for”这类过高频词，对上下文的信息贡献并不是很大，去除这些过高频词有助于提升训练速度。其公式如下所示P(w^{i}) = 1 - \\sqrt{\\frac{t}{f(w_{i})}}，其中t表示阈值参数，f表示单词频数，P表示该单词被去除的概率\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "# print(list(word_counts.items())[0])  \n",
    "\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "# 通过二次下采样公式去除相关的高频词，得到新的训练集词语列表\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "# print(train_words[:30])\n",
    "# 新的词语ID号码有所改变，如下所示，可以看出相关的the等过高频词已经去除 [8510, 19, 8511, 8, 3, 2460, 3047, 7185, 2461, 2154, 8511, 439, 3647, 3133, 4, 5848, 4204, 5849, 30, 4047, 3133, 7186, 6679, 114, 2641, 7771, 2462, 2973, 2575, 5848]\n",
    "# print(len(Counter(train_words))) \n",
    "\n",
    "# 在使用skip-gram结构之前，需要明确传入网络的词语批次batch，需要定义窗口的大小以及窗口范围内的词语列表。\n",
    "\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "# int_text = [i for i in range(10)]\n",
    "# print('Input: ', int_text)\n",
    "idx=5 # 对词语进行测试，查看某个固定的单词附近，如果随机选取5个词语范围之内的窗口，是否可以得到附近的词语ID\n",
    "\n",
    "# target = get_target(int_text, idx=idx, window_size=5)\n",
    "# print('Target: ', target) \n",
    "# 通过Target我们进行了两侧尝试，一次得到的是[4,6]，这是窗口大小为1的情形，一次得到的是[3, 4, 6, 7]，这是窗口大小为3的情形。\n",
    "# (TF2-Torch) libo@libos-MacBook-Pro lesson09 % python3 Core_lesson09.py\n",
    "# Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# Target:  [4, 6]\n",
    "# (TF2-Torch) libo@libos-MacBook-Pro lesson09 % python3 Core_lesson09.py\n",
    "# Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# Target:  [3, 4, 6, 7]\n",
    "\n",
    "# 获取批次数据batch data，利用以上的get_target函数恢复目标上下文的窗口内词语\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # 全batch\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "\n",
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "# 通过cosine_similarity计算词向量的相似度\n",
    "\n",
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "\n",
    "    #这里随机抽取一些单词，通过cosine similarity计算与这些单词关系最近的单词\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # 嵌入向量的大小 |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    " \n",
    "    # 从(0, window) 和（1000， 1000+window）范围内随机挑选N个单词\n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities\n",
    "\n",
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # 为输入和输出单词定义嵌入层\n",
    "        self.in_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "\n",
    "        # 按照均匀分布对嵌入表格进行初始化\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        # 返回输入词嵌入向量\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        # 返回输出词嵌入向量\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # 平均采样单词\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # 从噪声分布中采样单词\n",
    "\n",
    "        noise_words = torch.multinomial(noise_dist,batch_size * n_samples, replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if model.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        \n",
    "        \n",
    "        noise_vectors = self.out_embed(noise_words).view(batch_size,n_samples,self.n_embed)\n",
    "        \n",
    "        return noise_vectors\n",
    "\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        #  输入的词向量需要是完整词的列向量\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # 输出的词向量需要是完整词的行向量\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        #  正确的log-sigmoid损失函数\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # 不正确的log-sigmoid损失函数\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # 在噪声向量样本上将损失值相加\n",
    "\n",
    "        # 获取平均的batch损失函数\n",
    "        return -(out_loss + noise_loss).mean()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 获取噪声分布\n",
    "\n",
    "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "# 模型初始化\n",
    "embedding_dim = 300\n",
    "model = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "\n",
    "# 使用定义好的损失函数\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # get our input, target batches\n",
    "    for input_words, target_words in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # input, outpt, and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "            print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
    "            valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "\n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0000d57a567f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mviz_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m380\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membed_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mviz_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/TF2-Torch/lib/python3.7/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \"\"\"\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TF2-Torch/lib/python3.7/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'barnes_hut'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             X = check_array(X, ensure_min_samples=2,\n\u001b[0;32m--> 693\u001b[0;31m                             dtype=[np.float32, np.float64])\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TF2-Torch/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TF2-Torch/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "viz_words = 380\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2-Torch] *",
   "language": "python",
   "name": "conda-env-TF2-Torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
